from dataclasses import dataclass
from typing import List, Tuple

import ml_dtypes # support for bfloat16 dtype
import numpy as np

import os
from pathlib import Path
import re
import subprocess
import shutil
import pytest

from iree.compiler.ir import Context, Module

from .versioned_fixtures import versioned_unhashable_object_fixture, versioned_static_file_fixture, versioned_generated_file_fixture, versioned_cached_data_fixture, versioned_hashable_object_fixture


TOPDIR = Path(__file__).parent.parent.parent.parent

MODELS_DIR = TOPDIR / 'tests/testdata/'
BUILD_DIR = Path(os.environ.get('IREE_BUILD_DIR', str(TOPDIR.parent / 'iree-build')))
SOC_BUILD_DIR = Path(os.environ.get('IREE_SOC_BUILD_DIR', str(TOPDIR.parent / 'iree-build-soc')))


def pytest_addoption(parser):
    parser.addoption("--debug-ir", nargs='?', const=True, default=False, help="Enable detailed IR dump of after each pass (optionally set the dump directory)")
    parser.addoption("--generate-hw-test-vectors", action="store_true", default=False, help="Generate hardware test vectors")
    parser.addoption("--hw-test-vector-output-dir", default=False, help="Store hardware test vectors generated by test_torqrt.py in this directory")
    parser.addoption("--extra-torq-compiler-options", default=[], help="Add these extra options when compiling with the torq backend (use quotes to pass multiple arguments)")
    parser.addoption("--extra-torq-runtime-options", default=[], help="Add these extra options when executing a module with the torq runtime (use quotes to pass multiple arguments)")
    parser.addoption("--no-phases-dump", action="store_true", default=False, help="Disable phase dumps in torq compiler")
    parser.addoption("--debug-torq-compiler", type=int, default=0, help="Run torq compiler under gdb")
    parser.addoption("--trace-buffers", action="store_true", default=False, help="Enable tracing of buffers in the torq runtime")
    parser.addoption("--runtime-hw-type", action="store", default="cmodel", help="Command separate list of target hw to use for torq tests (cmodel, aws-fpga)")


def pytest_generate_tests(metafunc):
    if 'runtime_hw_type' in metafunc.fixturenames:
        runtime_hw_type = metafunc.config.getoption("runtime_hw_type").split(",")
        metafunc.parametrize('runtime_hw_type', runtime_hw_type, indirect=True)


def _find_iree_tool(env_var, tool_name):
    """Find IREE tool binary, checking environment variable first, then BUILD_DIR fallback."""

    # Check environment variable first
    env_path = os.getenv(env_var)
    if env_path:
        return env_path
    
    # Fall back to BUILD_DIR based path
    fallback_path = BUILD_DIR / 'third_party/iree/tools' / tool_name
    if fallback_path.exists():
        return str(fallback_path)

    # Check if tool is in the path
    tool_path = shutil.which(tool_name)
    if tool_path:
        return tool_path

    raise FileNotFoundError(f"Could not find {tool_name}.")


@versioned_hashable_object_fixture
def iree_opt():
    return _find_iree_tool('IREE_OPT', 'iree-opt')


def get_debug_ir_params(request):
    """
    Creates the debug parameters for invoking iree-compile based on pytest options
    """

    if request.config.getoption("--debug-ir") is not False:
        if request.config.getoption("--debug-ir") is True:
            tmpdir = request.getfixturevalue("tmpdir")
            dump_path = str(tmpdir / 'ir')
        else:
            dump_path = (Path(request.config.rootdir) / request.config.getoption("--debug-ir"))

        return ['--mlir-print-ir-after-all', f'--mlir-print-ir-tree-dir={dump_path}']

    return []


def get_extra_torq_compiler_options(request):
    """
    Finds extra torq compiler options from pytest options
    """

    cmds = []

    if request.config.getoption("--extra-torq-compiler-options"):
        cmds.extend(request.config.getoption("--extra-torq-compiler-options").split(" "))

    cmds += get_debug_ir_params(request)

    return cmds

def get_input_type_options(input_path):
    """
    Auto-detects the pipeline suitable to run the MLIR input file
    """

    with open(input_path, 'r') as mlir_file:
        mlir_content = mlir_file.read()
    if "torch.onnx" in mlir_content:
        return ['--iree-input-type=onnx-torq']
    elif "torch." in mlir_content:
        return ['--iree-input-type=torch-torq']
    return []

def compile_torq(request, iree_compile, input_path, output_path, ext_options=[]):
    """
    Compiles an mlir file with torq backend
    """

    ext_options = ext_options + get_input_type_options(input_path)

    cmds = [str(TOPDIR) + '/scripts/torq-compile',
            str(iree_compile),
            str(input_path), '-o', str(output_path),            
            *ext_options,
            *get_extra_torq_compiler_options(request)]
    
    if not request.config.getoption("--no-phases-dump"):
        cmds.append('--dump-compilation-phases-to=' + str(output_path) + '-phases')

    if request.config.getoption("--generate-hw-test-vectors"):
        cmds.append(f'--torq-dump-descriptors-dir={output_path}-cfgdesc')

    if request.config.getoption("--trace-buffers"):
        cmds.append('--torq-enable-buffer-debug-info')

    gdb_port = request.config.getoption('--debug-torq-compiler')
    if gdb_port > 0:
        cmds = ['gdbserver', 'localhost:' + str(gdb_port)] + cmds

    print("Compiling for TORQ with: " + " ".join(cmds))

    with request.getfixturevalue("scenario_log").event("torq_compile"):
        subprocess.check_call(cmds, cwd=str(Path(output_path).parent))


def create_output_args(request, output_specs, tag):
    """
    Creates the output command line args to invoke iree-run-module
    """
    output_path_root = str(request.getfixturevalue("tmpdir") / f'output_{tag}')

    output_args = []
    output_paths = []

    for idx, tensor_type in enumerate(output_specs):
        output_path = f'{output_path_root}_{idx}.bin'
        output_paths.append(output_path)
        output_args.append(f'--output=@{output_path}')

    return output_args, output_paths


def load_outputs(output_specs, output_paths):
    """
    Reads the data saved as outputs from iree-run-module
    """
    output_data = []

    for idx, tensor_type in enumerate(output_specs): 
        with open(output_paths[idx], 'rb') as f:
            data = np.frombuffer(f.read(), dtype=get_dtype(tensor_type.fmt)
                                 ).reshape(tensor_type.shape)
            output_data.append(data)
            
    return output_data


def run_torq(request, iree_run_module, model_path, input_args, output_specs, ext_options=[], tag=''):
    """
    Runs the specified vmfb model using iree-run-module with torq backend / hal driver
    """

    output_args, output_paths = create_output_args(request, output_specs, "torq" + tag)

    cmds = [str(iree_run_module),
            '--device=torq',
            '--module=' + str(model_path),
            '--function=main',
            *output_args,
            *ext_options,
            *input_args]

    tv_dir = str(request.getfixturevalue("tmpdir") / f'output_torq{tag}') + "-tv"
    buffers_dir = str(request.getfixturevalue("tmpdir") / f'output_torq{tag}') + "-buffers"

    if request.config.getoption("--generate-hw-test-vectors"):
        cmds.append('--torq_desc_data_dir=' + str(model_path) + "-cfgdesc")
        cmds.append('--torq_dump_mem_data_dir=' + tv_dir)

    if request.config.getoption("--trace-buffers"):
        cmds.append('--torq_dump_buffers_dir=' + buffers_dir)

    if request.config.getoption("--extra-torq-runtime-options"):
        cmds.extend(request.config.getoption("--extra-torq-runtime-options").split(" "))

    print("Running for TORQ with: " + " ".join(cmds))

    with request.getfixturevalue("scenario_log").event("torq_run"):
        # FIXME: Depending on the platform and the model this will not be enough (tests with desc dumping are particularly slow).
        subprocess.check_call(cmds, timeout=60 * 15)

    if request.config.getoption("--generate-hw-test-vectors"):
        print(f"Generated test vectors in {tv_dir}")

    if request.config.getoption("--trace-buffers"):
        print("\nBuffer tracing enabled\n")
        print("Buffer trace will be available in: " + buffers_dir + "\n")
        print("To view the buffer trace run:")

        ir_path = ""
        ir_dir = str(model_path) + '-phases'
        if os.path.exists(ir_dir):            
            for irs in os.listdir(ir_dir):
                if irs.endswith('9.executable-targets.mlir'):
                    ir_path = str(Path(ir_dir) / irs)
                    break

        print(f"cd {TOPDIR} && streamlit run apps/buffer_viewer/buffer_viewer.py {buffers_dir} {ir_path}")
        print()
        
    return load_outputs(output_specs, output_paths)


def compile_llvmcpu(request, iree_compile, input_path, output_path, compiler_options=[]):
    """
    Compile a mlir file with llvm-cpu backend
    """

    compiler_options = compiler_options

    cmd = [str(iree_compile),
           '--iree-hal-target-backends=llvm-cpu',
           str(input_path),
           '-o', str(output_path),
           *compiler_options]

    print("Compiling for LLVMCPU with: " + " ".join(cmd))

    subprocess.check_call(cmd)


def run_llvmcpu(request, iree_run_module, model_path, input_args, output_specs,
                runtime_options=[], tag=''):
    """
    Run a mlir file with local-task (CPU) HAL driver
    """

    output_args, output_paths = create_output_args(request, output_specs, "llvm" + tag)

    cmd = [str(iree_run_module),
           '--device=local-task',
           '--module=' + str(model_path),
           '--function=main',
           *output_args,
           *input_args,
           *runtime_options]

    print("Running for LLVMCPU with: " + " ".join(cmd))

    subprocess.check_call(cmd)

    return load_outputs(output_specs, output_paths)


def get_dtype(name):
    """
    Returns the numpy dtype corresponding to the given MLIR type name
    """

    dict_types = {'i1': bool,
                  'i8': np.int8,
                  'i16': np.int16,
                  'i32': np.int32,              
                  'ui8': np.uint8,
                  'f16': np.float16,
                  'f32': np.float32,
                  'si16': np.int16,
                  'si64': np.int64,
                  'si32': np.int32,
                  'bf16': ml_dtypes.bfloat16
                  }
    
    if name in dict_types:
        return dict_types[name]
        
    raise ValueError(f"Unsupported dtype {name}")


def is_float_type(dtype):
    """
    Returns true if the given dtype is a floating point type (either numpy native or bfloat16)
    """
    return np.issubdtype(dtype, np.floating) or dtype == ml_dtypes.bfloat16


@dataclass
class TensorType:    
    """
    Represents the type of a tensor input or output of an MLIR model
    """

    shape: List[int]
    fmt: str
    
    def to_arg(self):
        return "x".join([str(x) for x in self.shape] + [self.fmt])

    @staticmethod
    def from_string(spec):
        *shape_str, fmt = spec.split('x')
        shape = [int(s) for s in shape_str]
        return TensorType(shape, fmt)


@dataclass
class MlirIoSpec:
    inputs: List[TensorType]
    outputs: List[TensorType]


@versioned_cached_data_fixture
def mlir_io_spec(request, mlir_model_file):

    with open(mlir_model_file, 'r') as mlir_file:
        mlir_content = mlir_file.read()

    module = Module.parse(mlir_content, Context())
    for op in module.body.operations:
        input_types = []
        output_types = []

        for region in op.regions:
            for block in region.blocks:
                for arg in block.arguments:
                    input_types.append(str(arg.type))

                for inner_op in block.operations:
                    if inner_op.name == "func.return":
                        for i, operand in enumerate(inner_op.operands):
                            output_types.append(str(operand.type))

    input_specs = []
    output_specs = []

    def get_torch_specs(input):
        match = re.match(r"!torch\.vtensor<\[(.*)\],(\w+)>", input)

        if match and match.group(1) == '':
            shape = []
        else:
            shape = [int(x) for x in match.group(1).split(',')] if match else None

        if match:
            dtype = match.group(2)
            return TensorType(shape, dtype)
        else:
            return None

    def get_tosa_specs(input):
        match = re.match(r"tensor<([^>]+)>", input)
        if match:
            return TensorType.from_string(match.group(1))
        else:
            return None

    for t in input_types:
        input_specs.append(get_torch_specs(t))
    for t in output_types:
        output_specs.append(get_torch_specs(t))

    if None in input_specs or None in output_specs:
        input_specs = []
        output_specs = []

        for t in input_types:
            input_specs.append(get_tosa_specs(t))
        for t in output_types:
            output_specs.append(get_tosa_specs(t))

    return MlirIoSpec(inputs=input_specs, outputs=output_specs)


@versioned_static_file_fixture
def torq_compiler():
    """
    This fixture returns the path to the torq compiler binary.

    It is automatically invalidated when the compiler binary mtime changes.
    """

    return _find_iree_tool('IREE_COMPILE', 'iree-compile')


@versioned_hashable_object_fixture
def llvmcpu_compiler():
    """
    This fixture returns the path to the compiler binary.

    It is not invalidated when the compiler binary mtime changes.
    """

    return _find_iree_tool('IREE_COMPILE', 'iree-compile')


@versioned_static_file_fixture
def torq_runtime():
    """
    This fixture returns the path to the torq runtime binary.

    It is automatically invalidated when the runtime binary mtime changes.
    """

    return _find_iree_tool('IREE_RUN_MODULE', 'iree-run-module')


@versioned_hashable_object_fixture
def llvmcpu_runtime():
    """
    This fixture returns the path to the runtime binary.

    It is not invalidated when the runtime binary mtime changes.
    """

    return _find_iree_tool('IREE_RUN_MODULE', 'iree-run-module')


@versioned_unhashable_object_fixture
def iree_input_data_args(iree_input_data, mlir_io_spec):
    input_args = []
    
    for i, tensor_type in enumerate(mlir_io_spec.inputs):        
        file_name = iree_input_data / f'in_rnd_{i}.bin'        
        input_args.append(f'--input={tensor_type.to_arg()}=@{file_name}')        

    return input_args


@versioned_generated_file_fixture("dir")
def iree_input_data(request, versioned_file, input_data):
    """
    Save the received test data for inference
    """

    versioned_file.mkdir(parents=True, exist_ok=True)

    for i, data in enumerate(input_data):
        file_name = versioned_file / f'in_rnd_{i}.bin'

        with open(file_name, 'wb') as f:
            f.write(data.tobytes())

        np.save(str(file_name) + '.npy', data)    


@pytest.fixture
def mlir_static_file(case_config):
    return case_config['mlir_file_name']


@pytest.fixture
def mlir_model_file(request, case_config):
    return request.getfixturevalue(case_config['mlir_model_file'])


@versioned_hashable_object_fixture
def runtime_hw_type(request):
    return request.param


@versioned_hashable_object_fixture
def torq_compiler_options(request, case_config):
    return case_config.get("torq_compiler_options", [])


@versioned_generated_file_fixture("vmfb")
def torq_compiled_model(versioned_file, torq_compiler_options, request, mlir_model_file, torq_compiler):
    compile_torq(request, torq_compiler, mlir_model_file, versioned_file, torq_compiler_options)
        

@versioned_hashable_object_fixture
def torq_runtime_options(case_config):
    return case_config.get("torq_runtime_options", [])


@versioned_cached_data_fixture
def torq_results(request, torq_compiled_model, iree_input_data_args, mlir_io_spec, torq_runtime, runtime_hw_type, torq_runtime_options):

    options = [ "--torq_hw_type=" + runtime_hw_type ] + torq_runtime_options

    return run_torq(request, torq_runtime, torq_compiled_model, iree_input_data_args, mlir_io_spec.outputs, options)


@versioned_generated_file_fixture("vmfb")
def llvmcpu_compiled_model(versioned_file, llvmcpu_compiler, request, mlir_model_file):
    compile_llvmcpu(request, llvmcpu_compiler, mlir_model_file, versioned_file)


@versioned_cached_data_fixture
def llvmcpu_reference_results(request, llvmcpu_runtime, llvmcpu_compiled_model, iree_input_data_args, mlir_io_spec):
    return run_llvmcpu(request, llvmcpu_runtime, llvmcpu_compiled_model, iree_input_data_args, mlir_io_spec.outputs)


@pytest.fixture
def input_data(request, case_config):
    return request.getfixturevalue(case_config['input_data'])


@versioned_cached_data_fixture
def random_uniform_input_data(request, mlir_io_spec):

    result = []

    rng = np.random.default_rng(1234)

    for inp_spec in mlir_io_spec.inputs:
        dtype = get_dtype(inp_spec.fmt)

        if not np.issubdtype(dtype, np.integer):
            raise ValueError("Requested random uniform integer data for float type")

        data = rng.integers(np.iinfo(dtype).min, np.iinfo(dtype).max, size=inp_spec.shape, dtype=dtype)

        result.append(data)

    return result


@versioned_cached_data_fixture
def tweaked_random_input_data(request, mlir_io_spec):
    rng = np.random.default_rng(1234)

    input_tensors = []

    for tensor_type in mlir_io_spec.inputs:

        dtype = get_dtype(tensor_type.fmt)

        # TODO: there are many issues when we enable the full range
        # for now we limit the range to avoid issues
        if dtype == np.uint8:
            random_range = (0, 80)
        else:
            random_range = (-40, 40)

        if is_float_type(dtype):
            tensor = rng.uniform(random_range[0],
                                    random_range[1], tensor_type.shape).astype(dtype)
        elif np.issubdtype(dtype, np.integer):
            tensor = rng.integers(random_range[0], random_range[1],
                                    tensor_type.shape, dtype=dtype)
        elif dtype is bool:
            tensor = rng.integers(0, 2, tensor_type.shape, dtype=dtype)
        else:
            raise ValueError(f"Unsupported dtype {dtype}")

        input_tensors.append(tensor)

    return input_tensors


def _list_mlir_files(path_name: Path):
    test_files = []

    for file in path_name.iterdir():
        if file.suffix == '.mlir' and not file.name.startswith('disable'):
            test_files.append(path_name / file)

    return test_files


def list_mlir_files(dir_name):
    """
    Creates pytest parameters for all mlir files in the specified testdata subdirectory    
    """

    test_files = []

    testdata_dir = TOPDIR / 'tests' / 'testdata' / dir_name

    test_files = _list_mlir_files(testdata_dir)

    extra_dir = TOPDIR / 'extras/tests/testdata' / dir_name

    if extra_dir.exists():
        test_files.extend(_list_mlir_files(extra_dir))

    return sorted(test_files)


def list_mlir_file_group(group_name):

    root_dirs = [ TOPDIR / 'tests', TOPDIR / 'extras' / 'tests' ]

    files = []

    for root_dir in root_dirs:

        test_data_group = root_dir / 'testdata' / ( group_name + '.group')

        if not test_data_group.exists():
            continue

        with open(test_data_group, 'r') as f:
            groups = [ x.strip() for x in f.readlines()]

        for group in groups:
            
            dir_name = root_dir / 'testdata' / group

            if not dir_name.exists():
                continue

            files.extend(_list_mlir_files(dir_name))

    return sorted(files)



@versioned_static_file_fixture
def static_mlir_model_file(case_config):
    return case_config["static_mlir_model_file"]    


@versioned_cached_data_fixture
def accept_zero_output(request, mlir_model_file):

    with open(mlir_model_file, 'r') as mlir_file:
        mlir_header = mlir_file.readline()

    return "// TORQ_ALLOW_ALL_ZERO: 1" == mlir_header.strip()
