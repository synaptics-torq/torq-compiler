from dataclasses import dataclass
from typing import List, Tuple

import ml_dtypes # support for bfloat16 dtype
import numpy as np

import os
from pathlib import Path
import re
import subprocess
import shutil
import pytest
import json

from iree.compiler.ir import Context, Module

from .aws_fpga import FpgaSession
from .versioned_fixtures import versioned_unhashable_object_fixture, versioned_static_file_fixture, versioned_generated_file_fixture, \
                                versioned_cached_data_fixture, versioned_hashable_object_fixture, versioned_generated_directory_fixture


TOPDIR = Path(__file__).parent.parent.parent.parent

MODELS_DIR = TOPDIR / 'tests/testdata/'
BUILD_DIR = Path(os.environ.get('IREE_BUILD_DIR', str(TOPDIR.parent / 'iree-build')))
SOC_BUILD_DIR = Path(os.environ.get('IREE_SOC_BUILD_DIR', str(TOPDIR.parent / 'iree-build-soc')))


def pytest_addoption(parser):
    parser.addoption("--debug-ir", nargs='?', const=True, default=False, help="Enable detailed IR dump of after each pass (optionally set the dump directory)")
    parser.addoption("--generate-hw-test-vectors", action="store_true", default=False, help="Generate hardware test vectors")
    parser.addoption("--hw-test-vector-output-dir", default=False, help="Store hardware test vectors generated by test_torqrt.py in this directory")
    parser.addoption("--extra-torq-compiler-options", default=[], help="Add these extra options when compiling with the torq backend (use quotes to pass multiple arguments)")
    parser.addoption("--extra-torq-runtime-options", default=[], help="Add these extra options when executing a module with the torq runtime (use quotes to pass multiple arguments)")
    parser.addoption("--no-phases-dump", action="store_true", default=False, help="Disable phase dumps in torq compiler")
    parser.addoption("--debug-torq-compiler", type=int, default=0, help="Run torq compiler under gdb")
    parser.addoption("--trace-buffers", action="store_true", default=False, help="Enable tracing of buffers in the torq runtime")
    parser.addoption("--torq-runtime-hw-type", action="store", default="sim", help="Command separate list of target hw to use for torq tests (sim, aws_fpga)")
    parser.addoption("--torq-chips", action="store", default="default", help="Command separate list of chips to compile models for")


def pytest_generate_tests(metafunc):
    if 'runtime_hw_type' in metafunc.fixturenames:

        runtime_hw_type = metafunc.config.getoption("torq_runtime_hw_type").split(",")

        metafunc.parametrize('runtime_hw_type', runtime_hw_type, indirect=True)

        if metafunc.config.getoption("torq_chips") == 'all':
            chips = get_latest_chips()
        elif metafunc.config.getoption("torq_chips").endswith('.group'):
            with open(Path(TOPDIR / 'extras' / 'chips' / metafunc.config.getoption("torq_chips")), 'r') as f:
                chips = f.read().splitlines()
        else:
            chips = metafunc.config.getoption("torq_chips").split(",")

        if len(chips) == 0:
            raise ValueError("No chips found for torq tests")

        metafunc.parametrize('chip_config', chips, indirect=True)


def get_latest_chips():
    configs_dir = TOPDIR / 'extras' / 'chips'

    return [ f.resolve().stem for f in configs_dir.iterdir() if f.stem.endswith('latest') ]


@versioned_hashable_object_fixture
def chip_config(request):

    if request.param == 'default':

        return {
            "target": "SL2610"
        }    


    config_file = TOPDIR / 'extras' / 'chips' / f'{request.param}.json'

    if not config_file.exists():
        return {
            "target": request.param
        }

    with open(TOPDIR / 'extras' / 'chips' / f'{request.param}.json', 'r') as f:
        return json.load(f)


def _find_iree_tool(env_var, tool_name):
    """Find IREE tool binary, checking environment variable first, then BUILD_DIR fallback."""

    # Check environment variable first
    env_path = os.getenv(env_var)
    if env_path:
        return env_path
    
    # Fall back to BUILD_DIR based path
    fallback_path = BUILD_DIR / 'third_party/iree/tools' / tool_name
    if fallback_path.exists():
        return str(fallback_path)

    # Check if tool is in the path
    tool_path = shutil.which(tool_name)
    if tool_path:
        return tool_path

    raise FileNotFoundError(f"Could not find {tool_name}.")


@versioned_hashable_object_fixture
def iree_opt():
    return _find_iree_tool('IREE_OPT', 'iree-opt')



def get_input_type_options(input_path):
    """
    Auto-detects the pipeline suitable to run the MLIR input file
    """

    with open(input_path, 'r') as mlir_file:
        mlir_content = mlir_file.read()
    if "torch.onnx" in mlir_content:
        return ['--iree-input-type=onnx-torq']
    elif "torch." in mlir_content:
        return ['--iree-input-type=torch-torq']
    return []


def create_output_args(output_path_root, output_specs):
    """
    Creates the output command line args to invoke iree-run-module
    """    

    output_args = []

    for output_path in create_output_paths(output_path_root, output_specs):
        output_args.append(f'--output=@{output_path}')

    return output_args


def create_output_paths(output_path_root, output_specs):
    """
    Creates the paths for the outputs of iree-run-module
    """    
    
    output_paths = []

    for idx, tensor_type in enumerate(output_specs):
        output_path = f'{output_path_root}/output_{idx}.bin'
        output_paths.append(output_path)

    return output_paths


def load_outputs(output_specs, output_paths):
    """
    Reads the data saved as outputs from iree-run-module
    """
    output_data = []

    for idx, tensor_type in enumerate(output_specs): 
        with open(output_paths[idx], 'rb') as f:
            data = np.frombuffer(f.read(), dtype=get_dtype(tensor_type.fmt)
                                 ).reshape(tensor_type.shape)
            output_data.append(data)
            
    return output_data


def get_dtype(name):
    """
    Returns the numpy dtype corresponding to the given MLIR type name
    """

    dict_types = {'i1': bool,
                  'i8': np.int8,
                  'i16': np.int16,
                  'i32': np.int32,              
                  'ui8': np.uint8,
                  'f16': np.float16,
                  'f32': np.float32,
                  'si16': np.int16,
                  'si64': np.int64,
                  'si32': np.int32,
                  'bf16': ml_dtypes.bfloat16
                  }
    
    if name in dict_types:
        return dict_types[name]
        
    raise ValueError(f"Unsupported dtype {name}")


def is_float_type(dtype):
    """
    Returns true if the given dtype is a floating point type (either numpy native or bfloat16)
    """
    return np.issubdtype(dtype, np.floating) or dtype == ml_dtypes.bfloat16


@dataclass
class TensorType:    
    """
    Represents the type of a tensor input or output of an MLIR model
    """

    shape: List[int]
    fmt: str
    
    def to_arg(self):
        return "x".join([str(x) for x in self.shape] + [self.fmt])

    @staticmethod
    def from_string(spec):
        *shape_str, fmt = spec.split('x')
        shape = [int(s) for s in shape_str]
        return TensorType(shape, fmt)


@dataclass
class MlirIoSpec:
    inputs: List[TensorType]
    outputs: List[TensorType]


@versioned_cached_data_fixture
def mlir_io_spec(request, mlir_model_file):

    with open(mlir_model_file, 'r') as mlir_file:
        mlir_content = mlir_file.read()

    module = Module.parse(mlir_content, Context())
    for op in module.body.operations:
        input_types = []
        output_types = []

        for region in op.regions:
            for block in region.blocks:
                for arg in block.arguments:
                    input_types.append(str(arg.type))

                for inner_op in block.operations:
                    if inner_op.name == "func.return":
                        for i, operand in enumerate(inner_op.operands):
                            output_types.append(str(operand.type))

    input_specs = []
    output_specs = []

    def get_torch_specs(input):
        match = re.match(r"!torch\.vtensor<\[(.*)\],(\w+)>", input)

        if match and match.group(1) == '':
            shape = []
        else:
            shape = [int(x) for x in match.group(1).split(',')] if match else None

        if match:
            dtype = match.group(2)
            return TensorType(shape, dtype)
        else:
            return None

    def get_tosa_specs(input):
        match = re.match(r"tensor<([^>]+)>", input)
        if match:
            return TensorType.from_string(match.group(1))
        else:
            return None

    for t in input_types:
        input_specs.append(get_torch_specs(t))
    for t in output_types:
        output_specs.append(get_torch_specs(t))

    if None in input_specs or None in output_specs:
        input_specs = []
        output_specs = []

        for t in input_types:
            input_specs.append(get_tosa_specs(t))
        for t in output_types:
            output_specs.append(get_tosa_specs(t))

    return MlirIoSpec(inputs=input_specs, outputs=output_specs)


@versioned_static_file_fixture
def torq_compiler():
    """
    This fixture returns the path to the torq compiler binary.

    It is automatically invalidated when the compiler binary mtime changes.
    """

    return _find_iree_tool('IREE_COMPILE', 'iree-compile')


@versioned_hashable_object_fixture
def llvmcpu_compiler():
    """
    This fixture returns the path to the compiler binary.

    It is not invalidated when the compiler binary mtime changes.
    """

    return _find_iree_tool('IREE_COMPILE', 'iree-compile')


@versioned_static_file_fixture
def torq_runtime():
    """
    This fixture returns the path to the torq runtime binary.

    It is automatically invalidated when the runtime binary mtime changes.
    """

    return _find_iree_tool('IREE_RUN_MODULE', 'iree-run-module')


@versioned_hashable_object_fixture
def llvmcpu_runtime():
    """
    This fixture returns the path to the runtime binary.

    It is not invalidated when the runtime binary mtime changes.
    """

    return _find_iree_tool('IREE_RUN_MODULE', 'iree-run-module')


@versioned_unhashable_object_fixture
def iree_input_data_args(iree_input_data, mlir_io_spec):
    input_args = []
    
    for i, tensor_type in enumerate(mlir_io_spec.inputs):        
        file_name = iree_input_data / f'in_rnd_{i}.bin'        
        input_args.append(f'--input={tensor_type.to_arg()}=@{file_name}')        

    return input_args


@versioned_generated_directory_fixture
def iree_input_data(request, versioned_dir, input_data):
    """
    Save the received test data for inference
    """

    for i, data in enumerate(input_data):
        file_name = versioned_dir / f'in_rnd_{i}.bin'

        with open(file_name, 'wb') as f:
            f.write(data.tobytes())

        np.save(str(file_name) + '.npy', data)


@pytest.fixture
def mlir_static_file(case_config):
    return case_config['mlir_file_name']


@pytest.fixture
def mlir_model_file(request, case_config):
    return request.getfixturevalue(case_config['mlir_model_file'])


@versioned_hashable_object_fixture
def runtime_hw_type(request):
    return request.param


@versioned_hashable_object_fixture
def torq_compiler_options(request, case_config):

    cmds = case_config.get("torq_compiler_options", [])

    if request.config.getoption("--extra-torq-compiler-options"):
        cmds.extend(request.config.getoption("--extra-torq-compiler-options").split(" "))
    
    if request.config.getoption("--trace-buffers"):
        cmds.append('--torq-enable-buffer-debug-info')

    gdb_port = request.config.getoption('--debug-torq-compiler')
    if gdb_port > 0:
        cmds = ['gdbserver', 'localhost:' + str(gdb_port)] + cmds

    return cmds


@versioned_hashable_object_fixture
def torq_compiler_timeout(request, case_config):
    return int(case_config.get("torq_compiler_timeout", 60 * 15))


@versioned_hashable_object_fixture
def enable_debug_ir(request):
    return request.config.getoption("--debug-ir")


@versioned_hashable_object_fixture
def enable_phases_dump(request):
    return not request.config.getoption("--no-phases-dump", False)


@versioned_generated_directory_fixture
def torq_compiled_model_dir(versioned_dir, torq_compiler_options, request, mlir_model_file, torq_compiler, chip_config, 
                            torq_compiler_timeout, enable_debug_ir, enable_hw_test_vectors, enable_phases_dump, runtime_hw_type):
    
    model_file = versioned_dir / 'model.vmfb'

    target = chip_config.get("target", "SL2610")

    cmds = [str(TOPDIR) + '/scripts/torq-compile',
            str(torq_compiler),
            str(mlir_model_file), '-o', str(model_file)]

    if target == "custom":
        cmds.append(f'--torq-hw={chip_config["lram_size"]}:{chip_config["slice_count"]}:{chip_config["tiling_memory"]}:'
                     f'{chip_config.get("css_features","")}:{chip_config.get("nss_features","")}')
    else:
        cmds.append(f'--torq-hw={target}')

    if enable_debug_ir is not False:
        if enable_debug_ir is True:            
            dump_path = versioned_dir / 'ir'
        else:
            dump_path = Path(request.config.rootdir) / enable_debug_ir

        cmds.extend(['--mlir-print-ir-after-all', f'--mlir-print-ir-tree-dir={dump_path}'])

    if enable_phases_dump:
        cmds.append(f'--dump-compilation-phases-to={versioned_dir}/phases')

    if enable_hw_test_vectors:
        cmds.append(f'--torq-dump-descriptors-dir={versioned_dir}/cfgdesc')

    # when the runtime is cmodel, we need to compile with qemu address map
    if runtime_hw_type == 'sim':
        cmds.append('--torq-css-qemu')

    cmds += get_input_type_options(mlir_model_file)

    cmds += torq_compiler_options
    
    print("Compiling for TORQ with: " + " ".join(cmds))

    with request.getfixturevalue("scenario_log").event("torq_compile"):
        subprocess.check_call(cmds, cwd=str(versioned_dir), timeout=torq_compiler_timeout)


@versioned_unhashable_object_fixture
def torq_compiled_model(torq_compiled_model_dir):
    return torq_compiled_model_dir / 'model.vmfb'


@versioned_unhashable_object_fixture
def torq_compiled_hw_descriptors(torq_compiled_model_dir):
    return torq_compiled_model_dir / 'cfgdesc'


@versioned_unhashable_object_fixture
def torq_compiled_model_phases(torq_compiled_model_dir):
    return torq_compiled_model_dir / 'phases'


@versioned_hashable_object_fixture
def torq_runtime_options(request, case_config):
    cmds = case_config.get("torq_runtime_options", [])

    if request.config.getoption("--extra-torq-runtime-options"):
        cmds.extend(request.config.getoption("--extra-torq-runtime-options").split(" "))

    return cmds


@versioned_hashable_object_fixture
def enable_torq_buffer_tracing(request):
    return request.config.getoption("--trace-buffers")


@versioned_hashable_object_fixture
def enable_hw_test_vectors(request):
    return request.config.getoption("--generate-hw-test-vectors")


@versioned_hashable_object_fixture
def torq_runtime_timeout(request, case_config):
    return int(case_config.get("torq_runtime_timeout", 60 * 15))


@versioned_generated_directory_fixture
def torq_results_dir(versioned_dir, request, torq_compiled_model, iree_input_data_args, mlir_io_spec, 
                        torq_runtime, runtime_hw_type, torq_runtime_options, enable_torq_buffer_tracing, 
                        enable_hw_test_vectors, torq_runtime_timeout, chip_config):

    output_args = create_output_args(versioned_dir, mlir_io_spec.outputs)

    cmds = [str(torq_runtime),
            '--device=torq',
            '--module=' + str(torq_compiled_model),
            '--function=main',
            *output_args,
            '--torq_hw_type=' + runtime_hw_type,
            *torq_runtime_options,
            *iree_input_data_args]

    tv_dir = versioned_dir / 'tv'
    buffers_dir = versioned_dir / 'buffers'

    if enable_hw_test_vectors:
        cmds.append('--torq_desc_data_dir=' + str(request.getfixturevalue("torq_compiled_hw_descriptors")))
        cmds.append('--torq_dump_mem_data_dir=' + tv_dir)

    if enable_torq_buffer_tracing:
        cmds.append('--torq_dump_buffers_dir=' + buffers_dir)

    print("Running for TORQ with: " + " ".join(cmds))
    
    if runtime_hw_type == 'aws_fpga':            
        with FpgaSession(chip_config['aws_fpga']) as fpga_session:
            with request.getfixturevalue("scenario_log").event("torq_run"):
                subprocess.check_call(cmds, timeout=torq_runtime_timeout)
    else:
        with request.getfixturevalue("scenario_log").event("torq_run"):
            # FIXME: Depending on the platform and the model this will not be enough (tests with desc dumping are particularly slow).
            subprocess.check_call(cmds, timeout=torq_runtime_timeout)

    if enable_hw_test_vectors:
        print(f"Generated test vectors in {tv_dir}")

    if enable_torq_buffer_tracing:
        print("\nBuffer tracing enabled\n")
        print("Buffer trace will be available in: " + buffers_dir + "\n")
        print("To view the buffer trace run:")

        ir_path = ""
        ir_dir = request.getfixturevalue("torq_compiled_model_phases")
        if os.path.exists(ir_dir):            
            for irs in os.listdir(ir_dir):
                if irs.endswith('9.executable-targets.mlir'):
                    ir_path = str(Path(ir_dir) / irs)
                    break

        print(f"cd {TOPDIR} && streamlit run apps/buffer_viewer/buffer_viewer.py {buffers_dir} {ir_path}")
        print()


@versioned_unhashable_object_fixture
def torq_results(request, torq_results_dir, mlir_io_spec):
    output_paths = create_output_paths(torq_results_dir, mlir_io_spec.outputs)
    return load_outputs(mlir_io_spec.outputs, output_paths)


@versioned_generated_file_fixture("vmfb")
def llvmcpu_compiled_model(versioned_file, llvmcpu_compiler, request, mlir_model_file):

    cmd = [str(llvmcpu_compiler),
           '--iree-hal-target-backends=llvm-cpu',
           str(mlir_model_file),
           '-o', str(versioned_file)]

    print("Compiling for LLVMCPU with: " + " ".join(cmd))

    subprocess.check_call(cmd)


@versioned_generated_directory_fixture
def llvmcpu_reference_results_dir(versioned_dir, request, llvmcpu_runtime, llvmcpu_compiled_model, iree_input_data_args, mlir_io_spec):

    output_args = create_output_args(versioned_dir, mlir_io_spec.outputs)

    cmd = [str(llvmcpu_runtime),
           '--device=local-task',
           '--module=' + str(llvmcpu_compiled_model),
           '--function=main',
           *output_args,
           *iree_input_data_args]

    print("Running for LLVMCPU with: " + " ".join(cmd))

    subprocess.check_call(cmd)
    

@versioned_unhashable_object_fixture
def llvmcpu_reference_results(llvmcpu_reference_results_dir, mlir_io_spec):
    output_paths = create_output_paths(llvmcpu_reference_results_dir, mlir_io_spec.outputs)
    return load_outputs(mlir_io_spec.outputs, output_paths)


@pytest.fixture
def input_data(request, case_config):
    return request.getfixturevalue(case_config['input_data'])


@versioned_cached_data_fixture
def random_uniform_input_data(request, mlir_io_spec):

    result = []

    rng = np.random.default_rng(1234)

    for inp_spec in mlir_io_spec.inputs:
        dtype = get_dtype(inp_spec.fmt)

        if not np.issubdtype(dtype, np.integer):
            raise ValueError("Requested random uniform integer data for float type")

        data = rng.integers(np.iinfo(dtype).min, np.iinfo(dtype).max, size=inp_spec.shape, dtype=dtype)

        result.append(data)

    return result


@versioned_cached_data_fixture
def tweaked_random_input_data(request, mlir_io_spec):
    rng = np.random.default_rng(1234)

    input_tensors = []

    for tensor_type in mlir_io_spec.inputs:

        dtype = get_dtype(tensor_type.fmt)

        # TODO: there are many issues when we enable the full range
        # for now we limit the range to avoid issues
        if dtype == np.uint8:
            random_range = (0, 80)
        else:
            random_range = (-40, 40)

        if is_float_type(dtype):
            tensor = rng.uniform(random_range[0],
                                    random_range[1], tensor_type.shape).astype(dtype)
        elif np.issubdtype(dtype, np.integer):
            tensor = rng.integers(random_range[0], random_range[1],
                                    tensor_type.shape, dtype=dtype)
        elif dtype is bool:
            tensor = rng.integers(0, 2, tensor_type.shape, dtype=dtype)
        else:
            raise ValueError(f"Unsupported dtype {dtype}")

        input_tensors.append(tensor)

    return input_tensors


def _list_mlir_files(path_name: Path):
    test_files = []

    for file in path_name.iterdir():
        if file.suffix == '.mlir' and not file.name.startswith('disable'):
            test_files.append(path_name / file)

    return test_files


def list_mlir_files(dir_name):
    """
    Creates pytest parameters for all mlir files in the specified testdata subdirectory    
    """

    test_files = []

    testdata_dir = TOPDIR / 'tests' / 'testdata' / dir_name

    test_files = _list_mlir_files(testdata_dir)

    extra_dir = TOPDIR / 'extras/tests/testdata' / dir_name

    if extra_dir.exists():
        test_files.extend(_list_mlir_files(extra_dir))

    return sorted(test_files)


def list_mlir_file_group(group_name):

    root_dirs = [ TOPDIR / 'tests', TOPDIR / 'extras' / 'tests' ]

    files = []

    for root_dir in root_dirs:

        test_data_group = root_dir / 'testdata' / ( group_name + '.group')

        if not test_data_group.exists():
            continue

        with open(test_data_group, 'r') as f:
            groups = [ x.strip() for x in f.readlines()]

        for group in groups:
            
            dir_name = root_dir / 'testdata' / group

            if not dir_name.exists():
                continue

            files.extend(_list_mlir_files(dir_name))

    return sorted(files)


@versioned_static_file_fixture
def static_mlir_model_file(case_config):
    return case_config["static_mlir_model_file"]    


@versioned_cached_data_fixture
def comparison_config_from_mlir(request, mlir_model_file):
    """
    Extract comparison configuration directives from MLIR file comments

    These values will update the default comparison configuration used in tests.
    """

    tols = {}

    for line in open(mlir_model_file).readlines():
        if "TORQ_" in line:
            _, directive, value = line.split()
            tols[directive[5:-1].lower()] = float(value)

    return tols
